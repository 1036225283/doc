# 马尔可夫决策过程

$$P^a_{ss'}=E(S_{t+1}=s^{'}|S_t=s,A_t=a)$$ 中$$s^{'}$$的概率仅与上一个状态$$s$$有关，与之前的状态无关

$$π(a|s)=P(A_t=a|S_t=s)$$ 在状态$$s$$采取的动作$$a$$的概率仅与当前状态$$s$$有关，与其他的要素无关

$$q_π(s,a)=E_π(G_t|S_t=s,A_t=a)=E_π(R_{t+1}+γR_{t+2}+γ^2R_{t+3}+...|S_t=s,A_t=a)$$ 

# MDP的价值函数与贝尔曼方程

$$v_π(s)=E_π(G_t|S_t=s)=$$$$E_π(R_{t+1}+γR_{t+2}+γ^2R_{t+3}+...|S_t=s)$$ 

​                                             $$E_π(R_{t+1}+γ(R_{t+2}+γR_{t+3}+...)|S_t=s)$$

​                                             $$E_π(R_{t+1}+γG_{t+1}|S_t=s)$$

​                                             $$E_π(R_{t+1}+γv_π(S_{t+1})|S_t=s)$$



$$q_π(s,a)=E_π(G_t|S_t=s,A_t=a)=E_π(R_{t+1}+γq_π(S_{t+1},A_{t+1})|S_t=s,A_t=a)$$ 



贝尔曼方程告诉我们：一个状态的价值由该状态的奖励以及后续状态价值按照一定的衰减比例组成

# 状态价值函数和动作价值函数的关系

$$v_π(s)=\sum_{a \in A}π(a|s)q_π(s,a)$$ 

