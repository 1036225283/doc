吴恩达的课

p33 6-1
    逻辑回归，也就是sigmoid函数，

p36 6-4
    逻辑回归使用均方损失函数，有过拟合，并给出了交叉熵损失函数
    if y = 1, lost = -log(a)    = -log(sigmoid(z))
        期望值是1，当实际值接近于1时，误差趋近为0，反之，误差趋近于无限大
    if y = 0, lost = -log(1-a)  = -log(1-sigmoid(z))
        期望值是0，当实际值接近于0时，误差趋近为0，反之，误差趋近与无限大



p37 6-5
    将6-4的lost合二为一,以及推倒过程
    lost = -y*log(a) - (1-y)log(1-a)

    求到 lost = (a - y) * z

p38 6-6
    优化梯度下降

p39 6-7
    逻辑回归多分类
    A B C 分别为[1,0,0] [0,1,0] [0,0,1]

p40 7-1

