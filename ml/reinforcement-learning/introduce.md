#强化学习

##强化学习和监督学习的区别
    强化学习和监督学习最大的区别是它是没有监督学习已经准备好的训练数据输出值的。强化学习只有奖励值，但是这个奖励值和监督学习的输出值不一样，它不是事先给出的，而是延后给出的，比如上面的例子里走路摔倒了才得到大脑的奖励值。同时，强化学习的每一步与时间顺序前后关系紧密。而监督学习的训练数据之间一般都是独立的，没有这种前后的依赖关系。
##强化学习的建模
$$ f(x,y,z) = 3y^2z \left( 3+\frac{7x+5}{1+y^2} \right) $$

____

$$x^{y^z}=(1+{\rm e}^x)^{-2xy^w}$$

agent首先会选择action $$A_t$$

$$S$$ ,   $$t$$时刻环境的状态$$S_t$$ 

$$A$$ ,  $$t$$ 时刻agent采取的动作$$A_t$$ 

$$R$$ 环境的reward，t时刻agent在$$S_t$$采取的动作$$A_t$$对应的奖励$$R_{t+1}$$会在$$t+1$$时刻得到

$$π(a|s)=P(A_t=a|S_t=s)$$ Agent在状态s时采取动作a的概率

$$v_π(s)=E_π(R_{t+1}+γR_{t+2}+γ^2R_{t+3}+...|S_t=s)$$ Agent 在策略 π和状态 s时，采取action后的价值value

$$γ$$ 奖励衰减因子，在$$[0,1]$$之间,如果是0，是贪婪法，如果是1，后续的奖励一视同仁

$$P^a_{ss'}$$ 状态转化机。 表示在状态s下采取动作a，转化到下一个状态$$s^{'}$$的概率

ϵ = 0.9 探索率，随机生成一个概率，小于ϵ时，选择价值最大的action,否则，随机选择一个动作

